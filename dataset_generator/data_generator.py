import os
import sys
import copy
sys.path.append("/Users/liuvivian/table_tuning_for_error_generating_task")
ROOT = "/Users/liuvivian/table_tuning_for_error_generating_task"
import json
import pandas as pd
import numpy as np
from multiprocessing import Pool
from tqdm import tqdm
from typing import Optional, Union
from table_task.base_table_task import BaseTableTask
from table_task.table_task_factory import TableTaskFactory
from table_serializer import TableSerializer
from concurrent.futures import ThreadPoolExecutor
import random
from collections import defaultdict

class DataGenerator:
    def __init__(self,
                 table_task: Union[str, BaseTableTask],
                 source_dir: str = os.path.join(ROOT, "source"),
                 sample_size: int = 5,
                 n_jobs: int = 1,
                 verbose: bool = True):
        if isinstance(table_task, str):
            self.table_task = TableTaskFactory.get_table_task(table_task, sample_size=sample_size)
        else:
            self.table_task = table_task
        self.source_dir = source_dir
        self.n_jobs = n_jobs
        self.verbose = verbose

    def generate_data(self, split_ratio: float = 0.8, single_file_test: Optional[str] = None,
                      use_fewshot: bool = False ):
        """
        è¯»å– `source/` ç›®å½•ä¸­çš„ `_annotation.json` æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆè®­ç»ƒæ•°æ®
        :param single_file_test: å¦‚æœæä¾›ï¼Œåˆ™åªå¤„ç†è¯¥æ–‡ä»¶
        """
        if single_file_test:
            self.print_log(f"Running test on single annotation file: {single_file_test}")
            return self._process_single_file(single_file_test, use_fewshot=use_fewshot)

        self.print_log("Scanning annotation files in source directory...")

        annotation_files = []
        # annotation_files= ['/Users/liuvivian/table_tuning_for_error_generating_task/source/Company/Company_annotation.jsonl']
        annotation_files = [
            '/Users/liuvivian/table_tuning_for_error_generating_task/source/hospital/hospital_annotation.jsonl']
        # print("æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨:", os.path.exists(self.source_dir))
        # print("source_dir å†…å®¹:", os.listdir(self.source_dir))
        # print("source_dir æ˜¯å¦æ˜¯ç›®å½•:", os.path.isdir(self.source_dir))
        # for root, _, files in os.walk(self.source_dir):
        #     for file in files:
        #         if file.endswith("_annotation.jsonl"):
        #             full_path = os.path.join(root, file)
        #             annotation_files.append(full_path)

        print(annotation_files)

        all_data = []
        for file in tqdm(annotation_files, desc="Processing annotation files", unit="file"):
            all_data.extend(self.process_annotation_file(file, use_fewshot=use_fewshot))

        train_data, test_data_zeroshot = self.split_data(all_data, split_ratio)


        # å¤„ç†train dataçš„Fewshot

        for i,item in enumerate(train_data):
            num_example = random.randint(0, 4)
            example_idxs = random.sample(range(len(train_data)), num_example)  # ç¡®ä¿ç´¢å¼•ä¸é‡å¤

            for j,idx in enumerate(example_idxs):
                if i == idx:
                    continue
                train_data[idx]['instruction'] += f"\nInput:\n{item['input']}\nOutput:\n{item['output']}\n\n"


        # å¤„ç†test dataçš„fewshotå’Œzeroshot
        test_data_fewshot = copy.deepcopy(test_data_zeroshot)  # æ·±æ‹·è´ï¼Œç¡®ä¿ä¿®æ”¹ä¸å½±å“åŸæ•°æ®

        for i,item in enumerate(test_data_fewshot):
            num_example = 2  # å›ºå®š few-shot ç¤ºä¾‹ä¸ªæ•°
            example_idxs = random.sample(range(len(test_data_fewshot)), num_example)  # ç¡®ä¿ç´¢å¼•ä¸é‡å¤

            for j,idx in enumerate(example_idxs):
                if i == idx:
                    continue
                else:
                    test_data_fewshot[idx]['instruction'] += f"\nInput:\n{item['input']}\nOutput:\n{item['output']}\n\n"


        # # **å†å¤„ç† Few-shot ç‰ˆæœ¬çš„æµ‹è¯•é›†**
        # for file in tqdm(annotation_files, desc="Processing Few-shot Test Data", unit="file"):
        #     test_data_fewshot.extend(self.process_annotation_file(file, use_fewshot=True))

        # **ä¿å­˜æ‰€æœ‰æ•°æ®**
        self.save_dataset(train_data, test_data_zeroshot, test_data_fewshot)

    def _process_single_file(self, file_path: str, use_fewshot: bool = False) -> pd.DataFrame:
        """ ä»…å¤„ç†å•ä¸ª `_annotation.json` æ–‡ä»¶ """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"âŒ File not found: {file_path}")

        self.print_log(f"ğŸ“„ Processing file: {file_path}")
        results = self.process_annotation_file(file_path, use_fewshot=use_fewshot)

        if results:
            return pd.DataFrame(results)
        else:
            self.print_log("âš ï¸ No valid data in file.")
            return pd.DataFrame(columns=["instruction", "input", "output"])

    def process_annotation_file(self, file_path: str, use_fewshot: bool = False):
        """ è§£æå•ä¸ª `_annotation.jsonl` æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆæ•°æ®ï¼ˆå¹¶è¡Œè§£æ JSONï¼‰ """
        self.print_log(f"Processing file: {file_path}")

        max_samples_per_type = 10000  # æ¯ç§ error_type çš„æœ€å¤§æ•°é‡
        error_type_dict = defaultdict(list)  # è®°å½•æ¯ç§ error_type çš„æ•°æ®

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()  # è¯»å–æ‰€æœ‰è¡Œï¼Œè·å–æ–‡ä»¶æ€»è¡Œæ•°

        # **å…ˆç»Ÿè®¡ & æŒ‰ error_type å½’ç±»**
        for line in lines:
            try:
                entry = json.loads(line)
                error_type = entry["error_type"]
                error_type_dict[error_type].append(line)  # ä»¥ error_type ä½œä¸º key å­˜å‚¨
            except json.JSONDecodeError as e:
                print(f"JSON è§£æé”™è¯¯ {file_path}: {e}")
                continue  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ

        # **ğŸ“Œ å¤„ç†è¶…è¿‡ 5000 æ¡çš„ error_type**
        filtered_lines = []
        for error_type, records in error_type_dict.items():
            if len(records) > max_samples_per_type:
                random.shuffle(records)  # å…ˆæ‰“ä¹±
                selected_records = records[:max_samples_per_type]  # é€‰å‰ 5000 æ¡
                print(f"ğŸ” {error_type}: {len(records)} records, selected {len(selected_records)}")
            else:
                selected_records = records  # ç›´æ¥ç”¨å…¨éƒ¨
            filtered_lines.extend(selected_records)  # åŠ å…¥æœ€ç»ˆæ•°æ®

        results = []
        with ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
            for data in tqdm(
                    executor.map(lambda line: self.safe_parse_json(line, file_path), filtered_lines),
                    total=len(filtered_lines), desc=f"Processing {os.path.basename(file_path)}", unit="entry"
            ):
                if data:
                    results.append(data)

        return results

    def safe_parse_json(self, line, file_path, fewshot_examples=None):
        """ å®‰å…¨è§£æ JSONï¼Œé˜²æ­¢å´©æºƒ """
        try:
            entry = json.loads(line)
            return self.generate_data_entry(entry, file_path, fewshot_examples)
        except json.JSONDecodeError as e:
            print(f"JSON è§£æé”™è¯¯ {file_path}: {e}")
            return None  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ

    def get_fewshot_samples(self, lines, file_path, max_samples=5):
        """ ä» `lines` é‡Œéšæœºé€‰å– `max_samples` ä½œä¸º Few-shot ç¤ºä¾‹ï¼Œå¹¶è½¬æ¢ä¸º (instruction, input, output) æ ¼å¼ """
        if len(lines) == 0:
            return []

        num_samples = min(len(lines), random.randint(0, max_samples))
        sampled_lines = random.sample(lines, num_samples)

        fewshot_examples = []
        for line in sampled_lines:
            try:
                entry = line
                processed_entry = self.generate_data_entry(entry, file_path)
                fewshot_examples.append(processed_entry)  # âœ… ç¡®ä¿å®ƒæœ‰ "input" å’Œ "output"
            except json.JSONDecodeError as e:
                print(f"JSON è§£æé”™è¯¯ {file_path}: {e}")
                continue

        return fewshot_examples

    def generate_data_entry(self, entry, file_path, fewshot_examples=None):
        """ ç”Ÿæˆ (instruction, input, output) ä¸‰å…ƒç»„ï¼Œå¹¶æ”¯æŒ Few-shot ç¤ºä¾‹ """
        error_type = entry["error_type"]

        # ç”Ÿæˆ instruction
        instruction = f"{self.table_task.get_task_descriptions(error_type)} {self.table_task.get_error_type_descriptions(error_type)}"

        # è§£æ input è¡¨æ ¼
        input_table = self.table_task.construct_input(entry, file_path)

        # è§£æ output
        output_json = self.table_task.construct_output(entry)

        return {
            "instruction":instruction,
            "input": input_table,
            "output": output_json
        }

    def split_data(self, data, train_ratio=0.8):
        """ æŒ‰æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† """
        random.shuffle(data)
        split_idx = int(len(data) * train_ratio)
        return data[:split_idx], data[split_idx:]

    def save_dataset(self, train_data, test_data_zeroshot, test_data_fewshot):
        """ ä¿å­˜è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆåŒæ—¶ä¿å­˜ Zero-shot å’Œ Few-shot æµ‹è¯•é›†ï¼‰ """
        os.makedirs("dataset/train", exist_ok=True)
        os.makedirs("dataset/test", exist_ok=True)
        task_name = self.table_task.__class__.__name__.replace("Task", "").lower()

        train_file = f"dataset/train/train_{task_name}_1.jsonl"
        test_file_zeroshot = f"dataset/test/test_{task_name}_zeroshot_1.jsonl"
        test_file_fewshot = f"dataset/test/test_{task_name}_fewshot_1.jsonl"

        # **ä¿å­˜è®­ç»ƒé›†**
        with open(train_file, "w", encoding="utf-8") as f:
            for entry in train_data:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

        # **ä¿å­˜ Zero-shot æµ‹è¯•é›†**
        with open(test_file_zeroshot, "w", encoding="utf-8") as f:
            for entry in test_data_zeroshot:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

        # **ä¿å­˜ Few-shot æµ‹è¯•é›†**
        with open(test_file_fewshot, "w", encoding="utf-8") as f:
            for entry in test_data_fewshot:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

        print(f"ä»»åŠ¡ {task_name} è®­ç»ƒé›†å’Œä¸¤ç§æµ‹è¯•é›†å·²ç”Ÿæˆï¼")

    def print_log(self, *args):
        """ æ‰“å°æ—¥å¿— """
        if self.verbose:
            print(*args)

